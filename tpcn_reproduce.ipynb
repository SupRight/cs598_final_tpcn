{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uH0QMSkAAMlv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from itertools import groupby, islice\n",
        "import numpy as np\n",
        "\n",
        "# bit hacky but passes checks and I don't have time to implement a neater solution\n",
        "lab_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 13, 15, 16, 18, 21, 22, 23, 24, 29, 32, 33, 34, 39, 40, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 62, 63, 67, 68, 69, 70, 71, 72, 75, 83, 84, 86]\n",
        "labs_to_keep = [0] + [(i + 1) for i in lab_indices] + [(i + 88) for i in lab_indices] + [-1]\n",
        "no_lab_indices = list(range(87))\n",
        "no_lab_indices = [x for x in no_lab_indices if x not in lab_indices]\n",
        "no_labs_to_keep = [0] + [(i + 1) for i in no_lab_indices] + [(i + 88) for i in no_lab_indices] + [-1]\n",
        "\n",
        "\n",
        "class eICUReader(object):\n",
        "\n",
        "    def __init__(self, data_path, device=None, labs_only=False, no_labs=False):\n",
        "        self._diagnoses_path = data_path + '/diagnoses.csv'\n",
        "        self._labels_path = data_path + '/labels.csv'\n",
        "        self._flat_path = data_path + '/flat.csv'\n",
        "        self._timeseries_path = data_path + '/timeseries.csv'\n",
        "        self._device = device\n",
        "        self.labs_only = labs_only\n",
        "        self.no_labs = no_labs\n",
        "        self._dtype = torch.cuda.FloatTensor if device.type == 'cuda' else torch.FloatTensor\n",
        "\n",
        "        self.labels = pd.read_csv(self._labels_path, index_col='patient')\n",
        "        self.flat = pd.read_csv(self._flat_path, index_col='patient')\n",
        "        self.diagnoses = pd.read_csv(self._diagnoses_path, index_col='patient')\n",
        "\n",
        "        # we minus 2 to calculate F because hour and time are not features for convolution\n",
        "        self.F = (pd.read_csv(self._timeseries_path, index_col='patient', nrows=1).shape[1] - 2)//2\n",
        "        self.D = self.diagnoses.shape[1]\n",
        "        self.no_flat_features = self.flat.shape[1]\n",
        "\n",
        "        self.patients = list(self.labels.index)\n",
        "        self.no_patients = len(self.patients)\n",
        "\n",
        "    def line_split(self, line):\n",
        "        return [float(x) for x in line.split(',')]\n",
        "\n",
        "    def pad_sequences(self, ts_batch):\n",
        "        seq_lengths = [len(x) for x in ts_batch]\n",
        "        max_len = max(seq_lengths)\n",
        "        padded = [patient + [[0] * (self.F * 2 + 2)] * (max_len - len(patient)) for patient in ts_batch]\n",
        "        if self.labs_only:\n",
        "            padded = np.array(padded)\n",
        "            padded = padded[:, :, labs_to_keep]\n",
        "        if self.no_labs:\n",
        "            padded = np.array(padded)\n",
        "            padded = padded[:, :, no_labs_to_keep]\n",
        "        padded = torch.tensor(padded, device=self._device).type(self._dtype).permute(0, 2, 1)  # B * (2F + 2) * T\n",
        "        padded[:, 0, :] /= 24  # scale the time into days instead of hours\n",
        "        mask = torch.zeros(padded[:, 0, :].shape, device=self._device).type(self._dtype)\n",
        "        for p, l in enumerate(seq_lengths):\n",
        "            mask[p, :l] = 1\n",
        "        return padded, mask, torch.tensor(seq_lengths).type(self._dtype)\n",
        "\n",
        "    def get_los_labels(self, labels, times, mask):\n",
        "        times = labels.unsqueeze(1).repeat(1, times.shape[1]) - times\n",
        "        # clamp any labels that are less than 30 mins otherwise it becomes too small when the log is taken\n",
        "        # make sure where there is no data the label is 0\n",
        "        return (times.clamp(min=1/48) * mask)\n",
        "\n",
        "    def get_mort_labels(self, labels, length):\n",
        "        repeated_labels = labels.unsqueeze(1).repeat(1, length)\n",
        "        return repeated_labels\n",
        "\n",
        "    def batch_gen(self, batch_size=8, time_before_pred=5):\n",
        "\n",
        "        # note that once the generator is finished, the file will be closed automatically\n",
        "        with open(self._timeseries_path, 'r') as timeseries_file:\n",
        "            # the first line is the feature names; we have to skip over this\n",
        "            self.timeseries_header = next(timeseries_file).strip().split(',')\n",
        "            # this produces a generator that returns a list of batch_size patient identifiers\n",
        "            patient_batches = (self.patients[pos:pos + batch_size] for pos in range(0, len(self.patients), batch_size))\n",
        "            # create a generator to capture a single patient timeseries\n",
        "            ts_patient = groupby(map(self.line_split, timeseries_file), key=lambda line: line[0])\n",
        "            # we loop through these batches, tracking the index because we need it to index the pandas dataframes\n",
        "            for i, batch in enumerate(patient_batches):\n",
        "                ts_batch = [[line[1:] for line in ts] for _, ts in islice(ts_patient, batch_size)]\n",
        "                padded, mask, seq_lengths = self.pad_sequences(ts_batch)\n",
        "                los_labels = self.get_los_labels(torch.tensor(self.labels.iloc[i*batch_size:(i+1)*batch_size,7].values, device=self._device).type(self._dtype), padded[:,0,:], mask)\n",
        "                mort_labels = self.get_mort_labels(torch.tensor(self.labels.iloc[i*batch_size:(i+1)*batch_size,5].values, device=self._device).type(self._dtype), length=mask.shape[1])\n",
        "\n",
        "                # we must avoid taking data before time_before_pred hours to avoid diagnoses and apache variable from the future\n",
        "                yield (padded,  # B * (2F + 2) * T\n",
        "                       mask[:, time_before_pred:],  # B * (T - time_before_pred)\n",
        "                       torch.tensor(self.diagnoses.iloc[i*batch_size:(i+1)*batch_size].values, device=self._device).type(self._dtype),  # B * D\n",
        "                       torch.tensor(self.flat.iloc[i*batch_size:(i+1)*batch_size].values.astype(float), device=self._device).type(self._dtype),  # B * no_flat_features\n",
        "                       los_labels[:, time_before_pred:],\n",
        "                       mort_labels[:, time_before_pred:],\n",
        "                       seq_lengths - time_before_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1fvarN7JAPIP"
      },
      "outputs": [],
      "source": [
        "datareader = eICUReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HEirQOfdBss5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bf7c6b-1029-4724-dcf5-68b5884f0ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lkT_ufbPD-d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43704714-6389-4fd0-c067-a1967a823717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['labels.csv', 'stays.txt', 'flat.csv', 'diagnoses.csv', 'timeseries.csv']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# Set the path to the directory\n",
        "data_path = '/content/drive/MyDrive/ads/eicu/train'\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(data_path)\n",
        "print(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H22rVGFKBXiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46483664-9bd5-4c6b-e953-d5dea6f6e77d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data_path = '/content/drive/MyDrive/ads/eicu/'\n",
        "# import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "train_datareader = datareader(\n",
        "                    data_path + 'train',\n",
        "                    device=device,\n",
        "                    labs_only=False,\n",
        "                    no_labs=False\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmwwS-52DoCn"
      },
      "outputs": [],
      "source": [
        "val_datareader = datareader(\n",
        "                  data_path + 'val',\n",
        "                  device=device,\n",
        "                  labs_only=False,\n",
        "                  no_labs=False\n",
        "                  )\n",
        "test_datareader = datareader(\n",
        "                  data_path + 'test',\n",
        "                  device=device,\n",
        "                  labs_only=False,\n",
        "                  no_labs=False\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datareader.no_patients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJgvXm505P1i",
        "outputId": "3a5ab591-9e18-4801-bc0f-eb54effd6811"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102577"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_datareader.no_patients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRDNSkL_5whj",
        "outputId": "11146c3f-13bf-4cac-958c-e99466c400fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21990"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datareader.no_patients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFok4RXd5znt",
        "outputId": "56004ae1-1abe-43db-9761-b5d898be45ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22106"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JAgiQg8BElqJ"
      },
      "outputs": [],
      "source": [
        "def best_global(c):\n",
        "    c['alpha'] = 100\n",
        "    if c['dataset'] == 'eICU':\n",
        "        c['main_dropout_rate'] = 0.45\n",
        "        c['last_linear_size'] = 17\n",
        "        c['diagnosis_size'] = 64\n",
        "        c['batch_norm'] = 'mybatchnorm'\n",
        "    elif c['dataset'] == 'MIMIC':\n",
        "        # diagnosis size does not apply for MIMIC since we don't have diagnoses\n",
        "        c['main_dropout_rate'] = 0\n",
        "        c['last_linear_size'] = 36\n",
        "        c['batch_norm'] = 'mybatchnorm'\n",
        "    return c\n",
        "\n",
        "def best_tpc(c):\n",
        "    c = best_global(c)\n",
        "    c['mode'] = 'test'\n",
        "    c['model_type'] = 'tpc'\n",
        "    if c['dataset'] == 'eICU':\n",
        "        if c['percentage_data'] == 6.25:\n",
        "            c['n_epochs'] = 8\n",
        "        elif c['task'] == 'mortality':\n",
        "            c['n_epochs'] = 6\n",
        "        else:\n",
        "            c['n_epochs'] = 15\n",
        "        c['batch_size'] = 32\n",
        "        c['n_layers'] = 9\n",
        "        c['kernel_size'] = 4\n",
        "        c['no_temp_kernels'] = 12\n",
        "        c['point_size'] = 13\n",
        "        c['learning_rate'] = 0.00226\n",
        "        c['temp_dropout_rate'] = 0.05\n",
        "        c['temp_kernels'] = [12] * 9 if not c['share_weights'] else [32] * 9\n",
        "        c['point_sizes'] = [13] * 9\n",
        "    elif c['dataset'] == 'MIMIC':\n",
        "        c['no_diag'] = True\n",
        "        c['n_epochs'] = 10 if c['task'] != 'mortality' else 6\n",
        "        c['batch_size'] = 8\n",
        "        c['batch_size_test'] = 8  # purely to keep experiment size small so I can run many in parallel\n",
        "        c['n_layers'] = 8\n",
        "        c['kernel_size'] = 5\n",
        "        c['no_temp_kernels'] = 11\n",
        "        c['point_size'] = 5\n",
        "        c['learning_rate'] = 0.00221\n",
        "        c['temp_dropout_rate'] = 0.05\n",
        "        c['temp_kernels'] = [11] * 8\n",
        "        c['point_sizes'] = [5] * 8\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BHpbZdK1E0Tb"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # ── general ─────────────────────────────\n",
        "    \"dataset\": \"eICU\",\n",
        "    \"disable_cuda\": False,\n",
        "    \"intermediate_reporting\": False,\n",
        "    \"batch_size_test\": 32,\n",
        "    \"shuffle_train\": False,\n",
        "    \"save_results_csv\": False,\n",
        "    \"percentage_data\": 100.0,\n",
        "    \"task\": \"LoS\",\n",
        "    \"mode\": \"train\",\n",
        "\n",
        "    # ── loss ────────────────────────────────\n",
        "    \"loss\": \"hdloss\",\n",
        "    \"sum_losses\": True,\n",
        "\n",
        "    # ── ablations / feature flags ───────────\n",
        "    \"labs_only\": False,\n",
        "    \"no_mask\": False,\n",
        "    \"no_diag\": False,    # stays False because dataset is eICU\n",
        "    \"no_labs\": False,\n",
        "    \"no_exp\": False,\n",
        "\n",
        "    # ── shared hyper‑parameters ─────────────\n",
        "    \"alpha\": 100,\n",
        "    \"main_dropout_rate\": 0.45,\n",
        "    \"L2_regularisation\": 0.0,\n",
        "    \"last_linear_size\": 17,\n",
        "    \"diagnosis_size\": 64,\n",
        "    \"batchnorm\": \"mybatchnorm\",\n",
        "\n",
        "    # ── TPC‑specific hyper‑parameters ───────\n",
        "    \"n_epochs\": 15,\n",
        "    \"batch_size\": 32,\n",
        "    \"n_layers\": 9,\n",
        "    \"kernel_size\": 4,\n",
        "    \"no_temp_kernels\": 12,\n",
        "    \"point_size\": 13,\n",
        "    \"learning_rate\": 0.00226,\n",
        "    \"temp_dropout_rate\": 0.05,\n",
        "    \"share_weights\": False,\n",
        "    \"no_skip_connections\": False,\n",
        "\n",
        "    # ── derived lists (one entry per layer) ─\n",
        "    \"temp_kernels\": [12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
        "    \"point_sizes\": [13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
        "}\n",
        "config['dataset'] = 'eICU'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XPmJpbrcFC1G"
      },
      "outputs": [],
      "source": [
        "config = best_tpc(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e5B2xJflFEBR"
      },
      "outputs": [],
      "source": [
        "no_train_batches = len(train_datareader.patients) / config[\"batch_size\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1TYxy3WQGnI_"
      },
      "outputs": [],
      "source": [
        "checkpoint_counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dMA5My2IGsNI"
      },
      "outputs": [],
      "source": [
        "model = None\n",
        "optimiser = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aCsDCKZsGvrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889e3cdd-14a7-4602-af40-8b98baf3238b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "293"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_datareader.D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QsT_CRApHpuf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import cat, exp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import pad\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "from types import SimpleNamespace\n",
        "\n",
        "\n",
        "###============== The main defining function of the TPC model is temp_pointwise() on line 403 ==============###\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Hybrid Loss Function combining MSE, MAE, and Regularization\n",
        "class HybridLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, beta=1.0, lambda_reg=0.01):\n",
        "        super(HybridLoss, self).__init__()\n",
        "        self.mse_loss = nn.MSELoss(reduction='none')  # Mean Squared Error\n",
        "        self.mae_loss = nn.L1Loss(reduction='none')  # Mean Absolute Error\n",
        "        self.lambda_reg = lambda_reg  # Regularization term (L2 regularization weight)\n",
        "        self.alpha = alpha  # Weight for MSE\n",
        "        self.beta = beta  # Weight for MAE\n",
        "\n",
        "    def forward(self, y_hat, y, mask, seq_length, sum_losses=False):\n",
        "        # Masking the predictions and labels where there is no data\n",
        "        y_hat = y_hat.where(mask, torch.zeros_like(y))\n",
        "        y = y.where(mask, torch.zeros_like(y))\n",
        "\n",
        "        # Calculating MSE and MAE\n",
        "        mse = self.mse_loss(y_hat, y)\n",
        "        mae = self.mae_loss(y_hat, y)\n",
        "\n",
        "        # Summing the MSE and MAE losses across the batch\n",
        "        loss = self.alpha * torch.sum(mse, dim=1) + self.beta * torch.sum(mae, dim=1)\n",
        "\n",
        "        # Regularization term (L2 penalty on the weights)\n",
        "        l2_reg = self.lambda_reg * torch.sum(torch.square(y_hat))  # Example of L2 regularization\n",
        "\n",
        "        # Adding the regularization term to the loss\n",
        "        loss = loss + l2_reg\n",
        "\n",
        "        # Normalizing by sequence length to ensure the loss is proportional to sequence length\n",
        "        if not sum_losses:\n",
        "            loss = loss / seq_length.clamp(min=1)\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "# Mean Squared Logarithmic Error (MSLE) loss\n",
        "class MSLELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MSLELoss, self).__init__()\n",
        "        self.squared_error = nn.MSELoss(reduction='none')\n",
        "\n",
        "    def forward(self, y_hat, y, mask, seq_length, sum_losses=False):\n",
        "        # the log(predictions) corresponding to no data should be set to 0\n",
        "        log_y_hat = y_hat.log().where(mask, torch.zeros_like(y))\n",
        "        # the we set the log(labels) that correspond to no data to be 0 as well\n",
        "        log_y = y.log().where(mask, torch.zeros_like(y))\n",
        "        # where there is no data log_y_hat = log_y = 0, so the squared error will be 0 in these places\n",
        "        loss = self.squared_error(log_y_hat, log_y)\n",
        "        loss = torch.sum(loss, dim=1)\n",
        "        if not sum_losses:\n",
        "            loss = loss / seq_length.clamp(min=1)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "# Mean Squared Error (MSE) loss\n",
        "class MSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MSELoss, self).__init__()\n",
        "        self.squared_error = nn.MSELoss(reduction='none')\n",
        "\n",
        "    def forward(self, y_hat, y, mask, seq_length, sum_losses=False):\n",
        "        # the predictions corresponding to no data should be set to 0\n",
        "        y_hat = y_hat.where(mask, torch.zeros_like(y))\n",
        "        # the we set the labels that correspond to no data to be 0 as well\n",
        "        y = y.where(mask, torch.zeros_like(y))\n",
        "        # where there is no data log_y_hat = log_y = 0, so the squared error will be 0 in these places\n",
        "        loss = self.squared_error(y_hat, y)\n",
        "        loss = torch.sum(loss, dim=1)\n",
        "        if not sum_losses:\n",
        "            loss = loss / seq_length.clamp(min=1)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class MyBatchNorm(_BatchNorm):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n",
        "                 track_running_stats=True):\n",
        "        super(MyBatchNorm, self).__init__(\n",
        "            num_features, eps, momentum, affine, track_running_stats)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self._check_input_dim(input)\n",
        "\n",
        "        # hack to work around model.eval() issue\n",
        "        if not self.training:\n",
        "            self.eval_momentum = 0  # set the momentum to zero when the model is validating\n",
        "\n",
        "        if self.momentum is None:\n",
        "            exponential_average_factor = 0.0\n",
        "        else:\n",
        "            exponential_average_factor = self.momentum if self.training else self.eval_momentum\n",
        "\n",
        "        if self.track_running_stats:\n",
        "            if self.num_batches_tracked is not None:\n",
        "                self.num_batches_tracked = self.num_batches_tracked + 1\n",
        "                if self.momentum is None:  # use cumulative moving average\n",
        "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
        "                else:  # use exponential moving average\n",
        "                    exponential_average_factor = self.momentum if self.training else self.eval_momentum\n",
        "\n",
        "        return F.batch_norm(\n",
        "            input, self.running_mean, self.running_var, self.weight, self.bias,\n",
        "            training=True, momentum=exponential_average_factor, eps=self.eps)  # set training to True so it calculates the norm of the batch\n",
        "\n",
        "\n",
        "class MyBatchNorm1d(MyBatchNorm):\n",
        "    def _check_input_dim(self, input):\n",
        "        if input.dim() != 2 and input.dim() != 3:\n",
        "            raise ValueError('expected 2D or 3D input (got {}D input)'.format(input.dim()))\n",
        "\n",
        "\n",
        "class EmptyModule(nn.Module):\n",
        "    def forward(self, X):\n",
        "        return X\n",
        "\n",
        "\n",
        "class TempPointConv(nn.Module):\n",
        "    def __init__(self, config, F=None, D=None, no_flat_features=None):\n",
        "        # --------------------------------------------------------------\n",
        "        # NEW: make both dict‑ and attribute‑style configs work\n",
        "        # --------------------------------------------------------------\n",
        "        if isinstance(config, dict):               # user passed a dict\n",
        "            config = SimpleNamespace(**config)     # wrap for dot access\n",
        "        # afterwards we can keep calling config.xxx everywhere.\n",
        "        # --------------------------------------------------------------\n",
        "\n",
        "        super(TempPointConv, self).__init__()\n",
        "\n",
        "        # --- copy all hyper‑parameters --------------------------------\n",
        "        self.task              = config.task\n",
        "        self.n_layers          = config.n_layers\n",
        "        self.model_type        = config.model_type\n",
        "        self.share_weights     = getattr(config, 'share_weights', False)\n",
        "        self.diagnosis_size    = config.diagnosis_size\n",
        "        self.main_dropout_rate = config.main_dropout_rate\n",
        "        self.temp_dropout_rate = config.temp_dropout_rate\n",
        "        self.kernel_size       = config.kernel_size\n",
        "        self.temp_kernels      = config.temp_kernels\n",
        "        self.point_sizes       = config.point_sizes\n",
        "        self.batchnorm         = config.batchnorm\n",
        "        self.last_linear_size  = config.last_linear_size\n",
        "        self.F                 = F\n",
        "        self.D                 = D\n",
        "        self.no_flat_features  = no_flat_features\n",
        "        self.no_diag           = config.no_diag\n",
        "        self.no_mask           = config.no_mask\n",
        "        self.no_exp            = config.no_exp\n",
        "        self.no_skip_connections = config.no_skip_connections\n",
        "        self.alpha             = config.alpha\n",
        "        self.momentum          = 0.01 if self.batchnorm == 'low_momentum' else 0.1\n",
        "\n",
        "        # --- layers & helpers ----------------------------------------\n",
        "        self.relu      = nn.ReLU()\n",
        "        self.sigmoid   = nn.Sigmoid()\n",
        "        self.hardtanh  = nn.Hardtanh(min_val=1/48, max_val=100)\n",
        "        self.msle_loss = MSLELoss()\n",
        "        self.mse_loss  = MSELoss()\n",
        "        self.bce_loss  = nn.BCELoss()\n",
        "        self.hb_loss   = HybridLoss(alpha=self.alpha, beta=1.0, lambda_reg=0.01)\n",
        "\n",
        "        self.main_dropout = nn.Dropout(p=self.main_dropout_rate)\n",
        "        self.temp_dropout = nn.Dropout(p=self.temp_dropout_rate)\n",
        "\n",
        "        self.remove_none   = lambda x: tuple(xi for xi in x if xi is not None)\n",
        "        self.empty_module  = EmptyModule()\n",
        "\n",
        "        if self.batchnorm in ['mybatchnorm', 'pointonly', 'temponly', 'low_momentum']:\n",
        "            self.batchnormclass = MyBatchNorm1d\n",
        "        elif self.batchnorm == 'default':\n",
        "            self.batchnormclass = nn.BatchNorm1d\n",
        "\n",
        "        # input:  B × D   →  B × diagnosis_size\n",
        "        self.diagnosis_encoder   = nn.Linear(self.D, self.diagnosis_size)\n",
        "\n",
        "        if self.batchnorm in ['mybatchnorm', 'pointonly', 'low_momentum', 'default']:\n",
        "            self.bn_diagnosis_encoder = self.batchnormclass(self.diagnosis_size, momentum=self.momentum)\n",
        "            self.bn_point_last_los    = self.batchnormclass(self.last_linear_size, momentum=self.momentum)\n",
        "            self.bn_point_last_mort   = self.batchnormclass(self.last_linear_size, momentum=self.momentum)\n",
        "        else:\n",
        "            self.bn_diagnosis_encoder = self.empty_module\n",
        "            self.bn_point_last_los    = self.empty_module\n",
        "            self.bn_point_last_mort   = self.empty_module\n",
        "\n",
        "        # input:  (B × T) × last_linear_size   →   (B × T) × 1\n",
        "        self.point_final_los  = nn.Linear(self.last_linear_size, 1)\n",
        "        self.point_final_mort = nn.Linear(self.last_linear_size, 1)\n",
        "\n",
        "        # --- backbone selection --------------------------------------\n",
        "        if self.model_type == 'tpc':\n",
        "            self.init_tpc()\n",
        "        elif self.model_type == 'temp_only':\n",
        "            self.init_temp()\n",
        "        elif self.model_type == 'pointwise_only':\n",
        "            self.init_pointwise()\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                'model_type must be one of {tpc, temp_only, pointwise_only}'\n",
        "            )\n",
        "\n",
        "    def init_tpc(self):\n",
        "\n",
        "        # non-module layer attributes\n",
        "        self.layers = []\n",
        "        for i in range(self.n_layers):\n",
        "            dilation = i * (self.kernel_size - 1) if i > 0 else 1  # dilation = 1 for the first layer, after that it captures all the information gathered by previous layers\n",
        "            temp_k = self.temp_kernels[i]\n",
        "            point_size = self.point_sizes[i]\n",
        "            self.update_layer_info(layer=i, temp_k=temp_k, point_size=point_size, dilation=dilation, stride=1)\n",
        "\n",
        "        # module layer attributes\n",
        "        self.create_temp_pointwise_layers()\n",
        "\n",
        "        # input shape: (B * T) * ((F + Zt) * (1 + Y) + diagnosis_size + no_flat_features)\n",
        "        # output shape: (B * T) * last_linear_size\n",
        "        input_size = (self.F + self.Zt) * (1 + self.Y) + self.diagnosis_size + self.no_flat_features\n",
        "        if self.no_diag:\n",
        "            input_size = input_size - self.diagnosis_size\n",
        "        if self.no_skip_connections:\n",
        "            input_size = self.F * self.Y + self.Z + self.diagnosis_size + self.no_flat_features\n",
        "        self.point_last_los = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "        self.point_last_mort = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def init_temp(self):\n",
        "\n",
        "        # non-module layer attributes\n",
        "        self.layers = []\n",
        "        for i in range(self.n_layers):\n",
        "            dilation = i * (self.kernel_size - 1) if i > 0 else 1  # dilation = 1 for the first layer, after that it captures all the information gathered by previous layers\n",
        "            temp_k = self.temp_kernels[i]\n",
        "            self.update_layer_info(layer=i, temp_k=temp_k, dilation=dilation, stride=1)\n",
        "\n",
        "        # module layer attributes\n",
        "        self.create_temp_only_layers()\n",
        "\n",
        "        # input shape: (B * T) * (F * (1 + Y) + diagnosis_size + no_flat_features)\n",
        "        # output shape: (B * T) * last_linear_size\n",
        "        input_size = self.F * (1 + self.Y) + self.diagnosis_size + self.no_flat_features\n",
        "        self.point_last_los = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "        self.point_last_mort = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "        return\n",
        "\n",
        "\n",
        "    def init_pointwise(self):\n",
        "\n",
        "        # non-module layer attributes\n",
        "        self.layers = []\n",
        "        for i in range(self.n_layers):\n",
        "            point_size = self.point_sizes[i]\n",
        "            self.update_layer_info(layer=i, point_size=point_size)\n",
        "\n",
        "        # module layer attributes\n",
        "        self.create_pointwise_only_layers()\n",
        "\n",
        "        # input shape: (B * T) * (Zt + 2F + 2 + no_flat_features + diagnosis_size)\n",
        "        # output shape: (B * T) * last_linear_size\n",
        "        if self.no_mask:\n",
        "            input_size = self.Zt + self.F + 2 + self.no_flat_features + self.diagnosis_size\n",
        "        else:\n",
        "            input_size = self.Zt + 2 * self.F + 2 + self.no_flat_features + self.diagnosis_size\n",
        "        self.point_last_los = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "        self.point_last_mort = nn.Linear(in_features=input_size, out_features=self.last_linear_size)\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def update_layer_info(self, layer=None, temp_k=None, point_size=None, dilation=None, stride=None):\n",
        "\n",
        "        self.layers.append({})\n",
        "        if point_size is not None:\n",
        "            self.layers[layer]['point_size'] = point_size\n",
        "        if temp_k is not None:\n",
        "            padding = [(self.kernel_size - 1) * dilation, 0]  # [padding_left, padding_right]\n",
        "            self.layers[layer]['temp_kernels'] = temp_k\n",
        "            self.layers[layer]['dilation'] = dilation\n",
        "            self.layers[layer]['padding'] = padding\n",
        "            self.layers[layer]['stride'] = stride\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def create_temp_pointwise_layers(self):\n",
        "\n",
        "        ### Notation used for tracking the tensor shapes ###\n",
        "\n",
        "        # Z is the number of extra features added by the previous pointwise layer (could be 0 if this is the first layer)\n",
        "        # Zt is the cumulative number of extra features that have been added by all previous pointwise layers\n",
        "        # Zt-1 = Zt - Z (cumulative number of extra features minus the most recent pointwise layer)\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "\n",
        "        self.layer_modules = nn.ModuleDict()\n",
        "\n",
        "        self.Y = 0\n",
        "        self.Z = 0\n",
        "        self.Zt = 0\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            temp_in_channels = (self.F + self.Zt) * (1 + self.Y) if i > 0 else 2 * self.F  # (F + Zt) * (Y + 1)\n",
        "            temp_out_channels = (self.F + self.Zt) * self.layers[i]['temp_kernels']  # (F + Zt) * temp_kernels\n",
        "            linear_input_dim = (self.F + self.Zt - self.Z) * self.Y + self.Z + 2 * self.F + 2 + self.no_flat_features  # (F + Zt-1) * Y + Z + 2F + 2 + no_flat_features\n",
        "            linear_output_dim = self.layers[i]['point_size']  # point_size\n",
        "            # correct if no_mask\n",
        "            if self.no_mask:\n",
        "                if i == 0:\n",
        "                    temp_in_channels = self.F\n",
        "                linear_input_dim = (self.F + self.Zt - self.Z) * self.Y + self.Z + self.F + 2 + self.no_flat_features  # (F + Zt-1) * Y + Z + F + 2 + no_flat_features\n",
        "\n",
        "            temp = nn.Conv1d(in_channels=temp_in_channels,  # (F + Zt) * (Y + 1)\n",
        "                             out_channels=temp_out_channels,  # (F + Zt) * Y\n",
        "                             kernel_size=self.kernel_size,\n",
        "                             stride=self.layers[i]['stride'],\n",
        "                             dilation=self.layers[i]['dilation'],\n",
        "                             groups=self.F + self.Zt)\n",
        "\n",
        "            point = nn.Linear(in_features=linear_input_dim, out_features=linear_output_dim)\n",
        "\n",
        "            # correct if no_skip_connections\n",
        "            if self.no_skip_connections:\n",
        "                temp_in_channels = self.F * self.Y if i > 0 else 2 * self.F  # F * Y\n",
        "                temp_out_channels = self.F * self.layers[i]['temp_kernels']  # F * temp_kernels\n",
        "                #linear_input_dim = self.F * self.Y + self.Z if i > 0 else 2 * self.F + 2 + self.no_flat_features  # (F * Y) + Z\n",
        "                linear_input_dim = self.Z if i > 0 else 2 * self.F + 2 + self.no_flat_features  # Z\n",
        "                temp = nn.Conv1d(in_channels=temp_in_channels,\n",
        "                                 out_channels=temp_out_channels,\n",
        "                                 kernel_size=self.kernel_size,\n",
        "                                 stride=self.layers[i]['stride'],\n",
        "                                 dilation=self.layers[i]['dilation'],\n",
        "                                 groups=self.F)\n",
        "\n",
        "                point = nn.Linear(in_features=linear_input_dim, out_features=linear_output_dim)\n",
        "\n",
        "            if self.batchnorm in ['default', 'mybatchnorm', 'low_momentum']:\n",
        "                bn_temp = self.batchnormclass(num_features=temp_out_channels, momentum=self.momentum)\n",
        "                bn_point = self.batchnormclass(num_features=linear_output_dim, momentum=self.momentum)\n",
        "            elif self.batchnorm == 'temponly':\n",
        "                bn_temp = self.batchnormclass(num_features=temp_out_channels)\n",
        "                bn_point = self.empty_module\n",
        "            elif self.batchnorm == 'pointonly':\n",
        "                bn_temp = self.empty_module\n",
        "                bn_point = self.batchnormclass(num_features=linear_output_dim)\n",
        "            else:\n",
        "                bn_temp = bn_point = self.empty_module  # linear module; does nothing\n",
        "\n",
        "            self.layer_modules[str(i)] = nn.ModuleDict({\n",
        "                'temp': temp,\n",
        "                'bn_temp': bn_temp,\n",
        "                'point': point,\n",
        "                'bn_point': bn_point})\n",
        "\n",
        "            self.Y = self.layers[i]['temp_kernels']\n",
        "            self.Z = linear_output_dim\n",
        "            self.Zt += self.Z\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def create_temp_only_layers(self):\n",
        "\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "        self.layer_modules = nn.ModuleDict()\n",
        "        self.Y = 0\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            if self.share_weights:\n",
        "                temp_in_channels = (1 + self.Y) if i > 0 else 2  # (Y + 1)\n",
        "                temp_out_channels = self.layers[i]['temp_kernels']\n",
        "                groups = 1\n",
        "            else:\n",
        "                temp_in_channels = self.F * (1 + self.Y) if i > 0 else 2 * self.F  # F * (Y + 1)\n",
        "                temp_out_channels = self.F * self.layers[i]['temp_kernels']  # F * temp_kernels\n",
        "                groups = self.F\n",
        "\n",
        "            temp = nn.Conv1d(in_channels=temp_in_channels,\n",
        "                             out_channels=temp_out_channels,\n",
        "                             kernel_size=self.kernel_size,\n",
        "                             stride=self.layers[i]['stride'],\n",
        "                             dilation=self.layers[i]['dilation'],\n",
        "                             groups=groups)\n",
        "\n",
        "            if self.batchnorm in ['default', 'mybatchnorm', 'low_momentum', 'temponly']:\n",
        "                bn_temp = self.batchnormclass(num_features=temp_out_channels, momentum=self.momentum)\n",
        "            else:\n",
        "                bn_temp = self.empty_module  # linear module; does nothing\n",
        "\n",
        "            self.layer_modules[str(i)] = nn.ModuleDict({\n",
        "                'temp': temp,\n",
        "                'bn_temp': bn_temp})\n",
        "\n",
        "            self.Y = self.layers[i]['temp_kernels']\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def create_pointwise_only_layers(self):\n",
        "\n",
        "        # Zt is the cumulative number of extra features that have been added by previous pointwise layers\n",
        "        self.layer_modules = nn.ModuleDict()\n",
        "        self.Zt = 0\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            linear_input_dim = self.Zt + 2 * self.F + 2 + self.no_flat_features  # Zt + 2F + 2 + no_flat_features\n",
        "            linear_output_dim = self.layers[i]['point_size']  # point_size\n",
        "\n",
        "            if self.no_mask:\n",
        "                linear_input_dim = self.Zt + self.F + 2 + self.no_flat_features  # Zt + 2F + 2 + no_flat_features\n",
        "\n",
        "            point = nn.Linear(in_features=linear_input_dim, out_features=linear_output_dim)\n",
        "\n",
        "            if self.batchnorm in ['default', 'mybatchnorm', 'low_momentum', 'pointonly']:\n",
        "                bn_point = self.batchnormclass(num_features=linear_output_dim, momentum=self.momentum)\n",
        "            else:\n",
        "                bn_point = self.empty_module  # linear module; does nothing\n",
        "\n",
        "            self.layer_modules[str(i)] = nn.ModuleDict({\n",
        "                'point': point,\n",
        "                'bn_point': bn_point})\n",
        "\n",
        "            self.Zt += linear_output_dim\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    # This is really where the crux of TPC is defined. This function defines one TPC layer, as in Figure 3 in the paper:\n",
        "    # https://arxiv.org/pdf/2007.09483.pdf\n",
        "    def temp_pointwise(self, B=None, T=None, X=None, repeat_flat=None, X_orig=None, temp=None, bn_temp=None, point=None,\n",
        "                       bn_point=None, temp_kernels=None, point_size=None, padding=None, prev_temp=None, prev_point=None,\n",
        "                       point_skip=None):\n",
        "\n",
        "        ### Notation used for tracking the tensor shapes ###\n",
        "\n",
        "        # Z is the number of extra features added by the previous pointwise layer (could be 0 if this is the first layer)\n",
        "        # Zt is the cumulative number of extra features that have been added by all previous pointwise layers\n",
        "        # Zt-1 = Zt - Z (cumulative number of extra features minus the most recent pointwise layer)\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "        # X shape: B * ((F + Zt) * (Y + 1)) * T; N.B exception in the first layer where there are also mask features, in this case it is B * 2F * T\n",
        "        # repeat_flat shape: (B * T) * no_flat_features\n",
        "        # X_orig shape: (B * T) * (2F + 2)\n",
        "        # prev_temp shape: (B * T) * ((F + Zt-1) * (Y + 1))\n",
        "        # prev_point shape: (B * T) * Z\n",
        "\n",
        "        Z = prev_point.shape[1] if prev_point is not None else 0\n",
        "\n",
        "        X_padded = pad(X, padding, 'constant', 0)  # B * ((F + Zt) * (Y + 1)) * (T + padding)\n",
        "        X_temp = self.temp_dropout(bn_temp(temp(X_padded)))  # B * ((F + Zt) * temp_kernels) * T\n",
        "\n",
        "        X_concat = cat(self.remove_none((prev_temp,  # (B * T) * ((F + Zt-1) * Y)\n",
        "                                         prev_point,  # (B * T) * Z\n",
        "                                         X_orig,  # (B * T) * (2F + 2)\n",
        "                                         repeat_flat)),  # (B * T) * no_flat_features\n",
        "                       dim=1)  # (B * T) * (((F + Zt-1) * Y) + Z + 2F + 2 + no_flat_features)\n",
        "\n",
        "        point_output = self.main_dropout(bn_point(point(X_concat)))  # (B * T) * point_size\n",
        "\n",
        "        # point_skip input: B * (F + Zt-1) * T\n",
        "        # prev_point: B * Z * T\n",
        "        # point_skip output: B * (F + Zt) * T\n",
        "        point_skip = cat((point_skip, prev_point.view(B, T, Z).permute(0, 2, 1)), dim=1) if prev_point is not None else point_skip\n",
        "\n",
        "        temp_skip = cat((point_skip.unsqueeze(2),  # B * (F + Zt) * 1 * T\n",
        "                         X_temp.view(B, point_skip.shape[1], temp_kernels, T)),  # B * (F + Zt) * temp_kernels * T\n",
        "                        dim=2)  # B * (F + Zt) * (1 + temp_kernels) * T\n",
        "\n",
        "        X_point_rep = point_output.view(B, T, point_size, 1).permute(0, 2, 3, 1).repeat(1, 1, (1 + temp_kernels), 1)  # B * point_size * (1 + temp_kernels) * T\n",
        "        X_combined = self.relu(cat((temp_skip, X_point_rep), dim=1))  # B * (F + Zt) * (1 + temp_kernels) * T\n",
        "        next_X = X_combined.view(B, (point_skip.shape[1] + point_size) * (1 + temp_kernels), T)  # B * ((F + Zt + point_size) * (1 + temp_kernels)) * T\n",
        "\n",
        "        temp_output = X_temp.permute(0, 2, 1).contiguous().view(B * T, point_skip.shape[1] * temp_kernels)  # (B * T) * ((F + Zt) * temp_kernels)\n",
        "\n",
        "        return (temp_output,  # (B * T) * ((F + Zt) * temp_kernels)\n",
        "                point_output,  # (B * T) * point_size\n",
        "                next_X,  # B * ((F + Zt) * (1 + temp_kernels)) * T\n",
        "                point_skip)  # for keeping track of the point skip connections; B * (F + Zt) * T\n",
        "\n",
        "\n",
        "    def temp(self, B=None, T=None, X=None, X_temp_orig=None, temp=None, bn_temp=None, temp_kernels=None, padding=None):\n",
        "\n",
        "        ### Notation used for tracking the tensor shapes ###\n",
        "\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "        # X shape: B * (F * (Y + 1)) * T; N.B exception in the first layer where there are also mask features, in this case it is B * 2F * T\n",
        "        # X_temp_orig shape: B * F * T\n",
        "\n",
        "        X_padded = pad(X, padding, 'constant', 0)  # B * (F * (Y + 1)) * (T + padding)\n",
        "\n",
        "        if self.share_weights:\n",
        "            _, C, padded_length = X_padded.shape\n",
        "            chans = int(C / self.F)\n",
        "            X_temp = self.temp_dropout(bn_temp(temp(X_padded.view(B * self.F, chans, padded_length)))).view(B, (self.F * temp_kernels), T)  # B * (F * temp_kernels) * T\n",
        "        else:\n",
        "            X_temp = self.temp_dropout(bn_temp(temp(X_padded)))  # B * (F * temp_kernels) * T\n",
        "\n",
        "        temp_skip = self.relu(cat((X_temp_orig.unsqueeze(2),  # B * F * 1 * T\n",
        "                                   X_temp.view(B, self.F, temp_kernels, T)),  # B * F * temp_kernels * T\n",
        "                                   dim=2))  # B * F * (1 + temp_kernels) * T\n",
        "\n",
        "        next_X = temp_skip.view(B, (self.F * (1 + temp_kernels)), T)  # B * (F * (1 + temp_kernels)) * T\n",
        "\n",
        "        return next_X  # B * (F * temp_kernels) * T\n",
        "\n",
        "\n",
        "    def point(self, B=None, T=None, X=None, repeat_flat=None, X_orig=None, point=None, bn_point=None, point_skip=None):\n",
        "\n",
        "        ### Notation used for tracking the tensor shapes ###\n",
        "\n",
        "        # Z is the number of extra features added by the previous pointwise layer (could be 0 if this is the first layer)\n",
        "        # Zt is the cumulative number of extra features that have been added by all previous pointwise layers\n",
        "        # Zt-1 = Zt - Z (cumulative number of extra features minus the most recent pointwise layer)\n",
        "        # X shape: B * (F + Zt) * T; N.B exception in the first layer where there are also mask features, in this case it is B * 2F * T\n",
        "        # repeat_flat shape: (B * T) * no_flat_features\n",
        "        # X_orig shape: (B * T) * (2F + 2)\n",
        "        # prev_point shape: (B * T) * Z\n",
        "\n",
        "        X_combined = cat((X, repeat_flat), dim=1)\n",
        "\n",
        "        X_point = self.main_dropout(bn_point(point(X_combined)))  # (B * T) * point_size\n",
        "\n",
        "        # point_skip input: B * Zt-1 * T\n",
        "        # prev_point: B * Z * T\n",
        "        # point_skip output: B * Zt * T\n",
        "        point_skip = cat(self.remove_none((point_skip, X_point.view(B, T, -1).permute(0, 2, 1))), dim=1)\n",
        "\n",
        "        # point_skip: B * Zt * T\n",
        "        # X_orig: (B * T) * (2F + 2)\n",
        "        # repeat_flat: (B * T) * no_flat_features\n",
        "        # next_X: (B * T) * (Zt + 2F + 2 + no_flat_features)\n",
        "        next_X = self.relu(cat((point_skip.permute(0, 2, 1).contiguous().view(B * T, -1), X_orig), dim=1))\n",
        "\n",
        "        return (next_X,  # (B * T) * (Zt + 2F + 2 + no_flat_features)\n",
        "                point_skip)  # for keeping track of the pointwise skip connections; B * Zt * T\n",
        "\n",
        "\n",
        "    def temp_pointwise_no_skip(self, B=None, T=None, temp=None, bn_temp=None, point=None, bn_point=None, padding=None, prev_temp=None,\n",
        "                               prev_point=None, temp_kernels=None, X_orig=None, repeat_flat=None):\n",
        "\n",
        "        ### Temporal component ###\n",
        "\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "        # prev_temp shape: B * (F * Y) * T; N.B exception in the first layer where there are also mask features, in this case it is B * 2F * T\n",
        "\n",
        "        X_padded = pad(prev_temp, padding, 'constant', 0)  # B * (F * Y) * (T + padding)\n",
        "        temp_output = self.relu(self.temp_dropout(bn_temp(temp(X_padded))))  # B * (F * temp_kernels) * T\n",
        "\n",
        "        ### Pointwise component ###\n",
        "\n",
        "        # prev_point shape: (B * T) * ((F * Y) + Z)\n",
        "        point_output = self.relu(self.main_dropout(bn_point(point(prev_point))))  # (B * T) * point_size\n",
        "\n",
        "        return (temp_output,  # B * (F * temp_kernels) * T\n",
        "                point_output)  # (B * T) * point_size\n",
        "\n",
        "\n",
        "    def forward(self, X, diagnoses, flat, time_before_pred=5):\n",
        "        # flat is B * no_flat_features\n",
        "        # diagnoses is B * D\n",
        "        # X is B * (2F + 2) * T\n",
        "        # X_mask is B * T\n",
        "        # (the batch is padded to the longest sequence, the + 2 is the time and the hour which are not for temporal convolution)\n",
        "\n",
        "        # Split the input X into features\n",
        "        X_separated = torch.split(X[:, 1:-1, :], self.F, dim=1)  # tuple ((B * F * T), (B * F * T))\n",
        "\n",
        "        # Get batch size, features, and time dimension\n",
        "        B, _, T = X_separated[0].shape\n",
        "\n",
        "        # Debug print the shapes of tensors\n",
        "        # print(f\"Shape of X_separated[0]: {X_separated[0].shape}\")\n",
        "        # print(f\"Shape of flat: {flat.shape}\")\n",
        "        # print(f\"Shape of diagnoses: {diagnoses.shape}\")\n",
        "\n",
        "        if self.model_type in ['pointwise_only', 'tpc']:\n",
        "            repeat_flat = flat.repeat_interleave(T, dim=0)  # (B * T) * no_flat_features\n",
        "\n",
        "            if self.no_mask:\n",
        "                # For no mask case, include time and hour in X_orig (skip first and last time columns)\n",
        "                X_orig = cat((X_separated[0],\n",
        "                              X[:, 0, :].unsqueeze(1),\n",
        "                              X[:, -1, :].unsqueeze(1)), dim=1).permute(0, 2, 1).contiguous().view(B * T, self.F + 2)  # (B * T) * (F + 2)\n",
        "            else:\n",
        "                X_orig = X.permute(0, 2, 1).contiguous().view(B * T, 2 * self.F + 2)  # (B * T) * (2F + 2)\n",
        "\n",
        "            repeat_args = {'repeat_flat': repeat_flat, 'X_orig': X_orig, 'B': B, 'T': T}\n",
        "\n",
        "            if self.model_type == 'tpc':\n",
        "                if self.no_mask:\n",
        "                    next_X = X_separated[0]\n",
        "                else:\n",
        "                    next_X = torch.stack(X_separated, dim=2).reshape(B, 2 * self.F, T)  # B * 2F * T\n",
        "                point_skip = X_separated[0]  # Keeps track of skip connections generated from linear layers; B * F * T\n",
        "                temp_output = None\n",
        "                point_output = None\n",
        "            else:  # pointwise only\n",
        "                next_X = X_orig\n",
        "                point_skip = None\n",
        "\n",
        "        elif self.model_type == 'temp_only':\n",
        "            next_X = torch.stack(X_separated, dim=2).view(B, 2 * self.F, T)  # B * 2F * T\n",
        "            X_temp_orig = X_separated[0]  # Skip connections for temp only model\n",
        "            repeat_args = {'X_temp_orig': X_temp_orig, 'B': B, 'T': T}\n",
        "\n",
        "        if self.no_skip_connections:\n",
        "            temp_output = next_X\n",
        "            point_output = cat((X_orig,  # (B * T) * (2F + 2)\n",
        "                                repeat_flat),  # (B * T) * no_flat_features\n",
        "                              dim=1)  # (B * T) * (2F + 2 + no_flat_features)\n",
        "            self.layer1 = True\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            kwargs = dict(self.layer_modules[str(i)], **repeat_args)\n",
        "\n",
        "            if self.model_type == 'tpc':\n",
        "                if self.no_skip_connections:\n",
        "                    temp_output, point_output = self.temp_pointwise_no_skip(\n",
        "                        prev_point=point_output, prev_temp=temp_output,\n",
        "                        temp_kernels=self.layers[i]['temp_kernels'],\n",
        "                        padding=self.layers[i]['padding'], **kwargs)\n",
        "                else:\n",
        "                    temp_output, point_output, next_X, point_skip = self.temp_pointwise(\n",
        "                        X=next_X, point_skip=point_skip,\n",
        "                        prev_temp=temp_output, prev_point=point_output,\n",
        "                        temp_kernels=self.layers[i]['temp_kernels'],\n",
        "                        padding=self.layers[i]['padding'],\n",
        "                        point_size=self.layers[i]['point_size'], **kwargs)\n",
        "            elif self.model_type == 'temp_only':\n",
        "                next_X = self.temp(X=next_X, temp_kernels=self.layers[i]['temp_kernels'],\n",
        "                                  padding=self.layers[i]['padding'], **kwargs)\n",
        "            elif self.model_type == 'pointwise_only':\n",
        "                next_X, point_skip = self.point(X=next_X, point_skip=point_skip, **kwargs)\n",
        "\n",
        "        # Tidy up the outputs\n",
        "        if self.model_type == 'pointwise_only':\n",
        "            next_X = next_X.view(B, T, -1).permute(0, 2, 1)\n",
        "        elif self.no_skip_connections:\n",
        "            # Combine the final layer\n",
        "            next_X = cat((point_output,\n",
        "                          temp_output.permute(0, 2, 1).contiguous().view(B * T, self.F * self.layers[-1]['temp_kernels'])),\n",
        "                        dim=1)\n",
        "            next_X = next_X.view(B, T, -1).permute(0, 2, 1)\n",
        "\n",
        "        # Note: We cut off at time_before_pred hours here because the model is only valid from time_before_pred hours onwards\n",
        "        if self.no_diag:\n",
        "            combined_features = cat((flat.repeat_interleave(T - time_before_pred, dim=0),  # (B * (T - time_before_pred)) * no_flat_features\n",
        "                                    next_X[:, :, time_before_pred:].permute(0, 2, 1).contiguous().view(B * (T - time_before_pred), -1)), dim=1)  # (B * (T - time_before_pred)) * (((F + Zt) * (1 + Y)) + no_flat_features) for tpc\n",
        "        else:\n",
        "            diagnoses_enc = self.relu(self.main_dropout(self.bn_diagnosis_encoder(self.diagnosis_encoder(diagnoses))))  # B * diagnosis_size\n",
        "            combined_features = cat((flat.repeat_interleave(T - time_before_pred, dim=0),  # (B * (T - time_before_pred)) * no_flat_features\n",
        "                                    diagnoses_enc.repeat_interleave(T - time_before_pred, dim=0),  # (B * (T - time_before_pred)) * diagnosis_size\n",
        "                                    next_X[:, :, time_before_pred:].permute(0, 2, 1).contiguous().view(B * (T - time_before_pred), -1)), dim=1)  # (B * (T - time_before_pred)) * (((F + Zt) * (1 + Y)) + diagnosis_size + no_flat_features) for tpc\n",
        "\n",
        "        last_point_los = self.relu(self.main_dropout(self.bn_point_last_los(self.point_last_los(combined_features))))\n",
        "        last_point_mort = self.relu(self.main_dropout(self.bn_point_last_mort(self.point_last_mort(combined_features))))\n",
        "\n",
        "        if self.no_exp:\n",
        "            los_predictions = self.hardtanh(self.point_final_los(last_point_los).view(B, T - time_before_pred))  # B * (T - time_before_pred)\n",
        "        else:\n",
        "            los_predictions = self.hardtanh(exp(self.point_final_los(last_point_los).view(B, T - time_before_pred)))  # B * (T - time_before_pred)\n",
        "        mort_predictions = self.sigmoid(self.point_final_mort(last_point_mort).view(B, T - time_before_pred))  # B * (T - time_before_pred)\n",
        "\n",
        "        return los_predictions, mort_predictions\n",
        "\n",
        "\n",
        "\n",
        "    def temp_pointwise_no_skip_old(self, B=None, T=None, temp=None, bn_temp=None, point=None, bn_point=None, padding=None, prev_temp=None,\n",
        "                               prev_point=None, temp_kernels=None, X_orig=None, repeat_flat=None):\n",
        "\n",
        "        ### Temporal component ###\n",
        "\n",
        "        # Y is the number of channels in the previous temporal layer (could be 0 if this is the first layer)\n",
        "        # prev_temp shape: B * (F * Y) * T; N.B exception in the first layer where there are also mask features, in this case it is B * 2F * T\n",
        "\n",
        "        X_padded = pad(prev_temp, padding, 'constant', 0)  # B * (F * Y) * (T + padding)\n",
        "        temp_output = self.relu(self.temp_dropout(bn_temp(temp(X_padded))))  # B * (F * temp_kernels) * T\n",
        "\n",
        "        ### Pointwise component ###\n",
        "\n",
        "        # prev_point shape: (B * T) * ((F * Y) + Z)\n",
        "\n",
        "        # if this is not layer 1:\n",
        "        if self.layer1:\n",
        "            X_concat = prev_point\n",
        "            self.layer1 = False\n",
        "        else:\n",
        "            X_concat = cat((prev_point,\n",
        "                            prev_temp.permute(0, 2, 1).contiguous().view(B * T, self.F * temp_kernels)),\n",
        "                           dim=1)\n",
        "\n",
        "        point_output = self.relu(self.main_dropout(bn_point(point(X_concat))))  # (B * T) * point_size\n",
        "\n",
        "        return (temp_output,  # B * (F * temp_kernels) * T\n",
        "                point_output)  # (B * T) * point_size\n",
        "\n",
        "\n",
        "    def loss(self, y_hat_los, y_hat_mort, y_los, y_mort, mask, seq_lengths, device, sum_losses, loss_type):\n",
        "        # mort loss\n",
        "        if self.task == 'mortality':\n",
        "            loss = self.bce_loss(y_hat_mort, y_mort) * self.alpha\n",
        "        # los loss\n",
        "        else:\n",
        "            bool_type = torch.cuda.BoolTensor if device == torch.device('cuda') else torch.BoolTensor\n",
        "            if loss_type == 'msle':\n",
        "                los_loss = self.msle_loss(y_hat_los, y_los, mask.type(bool_type), seq_lengths, sum_losses)\n",
        "            elif loss_type == 'mse':\n",
        "                los_loss = self.mse_loss(y_hat_los, y_los, mask.type(bool_type), seq_lengths, sum_losses)\n",
        "            elif loss_type == 'hdloss':  # Add this condition to select the HybridLoss\n",
        "              los_loss = self.hb_loss(y_hat_los, y_los, mask.type(bool_type), seq_lengths, sum_losses)\n",
        "            if self.task == 'LoS':\n",
        "                loss = los_loss\n",
        "            # multitask loss\n",
        "            if self.task == 'multitask':\n",
        "                loss = los_loss + self.bce_loss(y_hat_mort, y_mort) * self.alpha\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-8MMRye2H9uh"
      },
      "outputs": [],
      "source": [
        "model = TempPointConv(\n",
        "    config=config,\n",
        "    F=train_datareader.F,\n",
        "    D=train_datareader.D,\n",
        "    no_flat_features= train_datareader.no_flat_features\n",
        "    ).to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2-QHkvuzITDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd46099-6d7b-4825-ea43-d9040fb3c959"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TempPointConv(\n",
              "  (relu): ReLU()\n",
              "  (sigmoid): Sigmoid()\n",
              "  (hardtanh): Hardtanh(min_val=0.020833333333333332, max_val=100)\n",
              "  (msle_loss): MSLELoss(\n",
              "    (squared_error): MSELoss()\n",
              "  )\n",
              "  (mse_loss): MSELoss(\n",
              "    (squared_error): MSELoss()\n",
              "  )\n",
              "  (bce_loss): BCELoss()\n",
              "  (hb_loss): HybridLoss(\n",
              "    (mse_loss): MSELoss()\n",
              "    (mae_loss): L1Loss()\n",
              "  )\n",
              "  (main_dropout): Dropout(p=0.45, inplace=False)\n",
              "  (temp_dropout): Dropout(p=0.05, inplace=False)\n",
              "  (empty_module): EmptyModule()\n",
              "  (diagnosis_encoder): Linear(in_features=293, out_features=64, bias=True)\n",
              "  (bn_diagnosis_encoder): MyBatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn_point_last_los): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn_point_last_mort): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (point_final_los): Linear(in_features=17, out_features=1, bias=True)\n",
              "  (point_final_mort): Linear(in_features=17, out_features=1, bias=True)\n",
              "  (layer_modules): ModuleDict(\n",
              "    (0): ModuleDict(\n",
              "      (temp): Conv1d(174, 1044, kernel_size=(4,), stride=(1,), groups=87)\n",
              "      (bn_temp): MyBatchNorm1d(1044, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=241, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): ModuleDict(\n",
              "      (temp): Conv1d(1300, 1200, kernel_size=(4,), stride=(1,), dilation=(3,), groups=100)\n",
              "      (bn_temp): MyBatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=1298, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): ModuleDict(\n",
              "      (temp): Conv1d(1469, 1356, kernel_size=(4,), stride=(1,), dilation=(6,), groups=113)\n",
              "      (bn_temp): MyBatchNorm1d(1356, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=1454, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): ModuleDict(\n",
              "      (temp): Conv1d(1638, 1512, kernel_size=(4,), stride=(1,), dilation=(9,), groups=126)\n",
              "      (bn_temp): MyBatchNorm1d(1512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=1610, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): ModuleDict(\n",
              "      (temp): Conv1d(1807, 1668, kernel_size=(4,), stride=(1,), dilation=(12,), groups=139)\n",
              "      (bn_temp): MyBatchNorm1d(1668, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=1766, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): ModuleDict(\n",
              "      (temp): Conv1d(1976, 1824, kernel_size=(4,), stride=(1,), dilation=(15,), groups=152)\n",
              "      (bn_temp): MyBatchNorm1d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=1922, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (6): ModuleDict(\n",
              "      (temp): Conv1d(2145, 1980, kernel_size=(4,), stride=(1,), dilation=(18,), groups=165)\n",
              "      (bn_temp): MyBatchNorm1d(1980, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=2078, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (7): ModuleDict(\n",
              "      (temp): Conv1d(2314, 2136, kernel_size=(4,), stride=(1,), dilation=(21,), groups=178)\n",
              "      (bn_temp): MyBatchNorm1d(2136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=2234, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (8): ModuleDict(\n",
              "      (temp): Conv1d(2483, 2292, kernel_size=(4,), stride=(1,), dilation=(24,), groups=191)\n",
              "      (bn_temp): MyBatchNorm1d(2292, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=2390, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (point_last_los): Linear(in_features=2781, out_features=17, bias=True)\n",
              "  (point_last_mort): Linear(in_features=2781, out_features=17, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mdD1P2A2I5J3"
      },
      "outputs": [],
      "source": [
        "train_batches = train_datareader.batch_gen(batch_size=config[\"batch_size\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbOSxxDaJlDW",
        "outputId": "0274800d-85a7-499c-ba55-37948abe6453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LoS Loss: 7.1211, Mortality Loss: 0.7035\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Mean Squared Error for Length of Stay (LoS) prediction (Regression Task)\n",
        "criterion_los = nn.MSELoss()\n",
        "\n",
        "# Binary Cross-Entropy with Logits for Mortality prediction (Binary Classification Task)\n",
        "criterion_mort = nn.BCEWithLogitsLoss()\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def train(model, train_datareader, optimizer, criterion_los, criterion_mort, device):\n",
        "    model.train()\n",
        "    running_loss_los = 0.0\n",
        "    running_loss_mort = 0.0\n",
        "    num_batches = 0  # To count the number of batches\n",
        "\n",
        "    # Iterate directly over the generator produced by batch_gen\n",
        "    for batch in train_datareader.batch_gen(batch_size=config['batch_size']):\n",
        "        try:\n",
        "            padded, mask, diagnoses, flat, los_labels, mort_labels, seq_lengths = batch  # Unpack correctly\n",
        "\n",
        "            # Move data to the device (GPU/CPU)\n",
        "            padded, los_labels, mort_labels = padded.to(device), los_labels.to(device), mort_labels.to(device)\n",
        "            flat, diagnoses = flat.to(device), diagnoses.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            los_pred, mort_pred = model(padded, diagnoses, flat)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss_los = criterion_los(los_pred, los_labels)\n",
        "            loss_mort = criterion_mort(mort_pred, mort_labels)\n",
        "\n",
        "            # Total loss\n",
        "            loss = loss_los + loss_mort\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss_los += loss_los.item()\n",
        "            running_loss_mort += loss_mort.item()\n",
        "            num_batches += 1  # Increment the batch counter\n",
        "            # print(f'Batch {num_batches}: LoS Loss: {loss_los.item():.4f}, Mortality Loss: {loss_mort.item():.4f}')\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"Skipping batch due to error: {e}\")\n",
        "            continue  # Skip this batch and proceed to the next one\n",
        "\n",
        "    # Calculate the average loss\n",
        "    avg_loss_los = running_loss_los / num_batches if num_batches > 0 else 0\n",
        "    avg_loss_mort = running_loss_mort / num_batches if num_batches > 0 else 0\n",
        "\n",
        "    return avg_loss_los, avg_loss_mort\n",
        "\n",
        "\n",
        "# Example of training for one epoch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# for epoch in range(10):  # Run for 10 epochs as an example\n",
        "train_loss_los, train_loss_mort = train(model, train_datareader, optimizer, criterion_los, criterion_mort, device)\n",
        "print(f' LoS Loss: {train_loss_los:.4f}, Mortality Loss: {train_loss_mort:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZQTyV-PgJmz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1fab71-8ba3-40cd-f6df-93442c2b7d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'MAD (LoS)': np.float32(0.64354044), 'MSE (LoS)': np.float64(10.929902297026382), 'MSLE (LoS)': np.float64(0.12085423469097314), 'R2 (LoS)': np.float64(0.05843460618039), 'Accuracy (Mortality)': np.float64(0.9389165419161677), 'Kappa (Mortality)': np.float64(0.3386)}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
        "\n",
        "# Store per-batch metrics\n",
        "batch_metrics = []\n",
        "\n",
        "# Iterate through the validation data\n",
        "for batch_idx, batch in enumerate(val_datareader.batch_gen(batch_size=config['batch_size'])):\n",
        "    padded, mask, diagnoses, flat, los_labels, mort_labels, seq_lengths = batch\n",
        "\n",
        "    # Move data to device (GPU/CPU)\n",
        "    padded, los_labels, mort_labels = padded.to(device), los_labels.to(device), mort_labels.to(device)\n",
        "    flat, diagnoses = flat.to(device), diagnoses.to(device)\n",
        "\n",
        "    try:\n",
        "        # Forward pass: Get model predictions\n",
        "        los_pred, mort_pred = model(padded, diagnoses, flat)\n",
        "\n",
        "        # Ensure binary predictions (0 or 1) by thresholding the predicted values\n",
        "        mort_pred_binary = (mort_pred.detach().cpu().numpy() > 0.5).astype(int)  # Apply threshold to get binary (0 or 1)\n",
        "\n",
        "        # Ensure labels are binary (0 or 1)\n",
        "        mort_labels_binary = mort_labels.detach().cpu().numpy().astype(int)\n",
        "\n",
        "        # Flatten if the predictions or labels are not 1D arrays\n",
        "        if len(mort_labels_binary.shape) > 1:\n",
        "            mort_labels_binary = mort_labels_binary.flatten()\n",
        "\n",
        "        if len(mort_pred_binary.shape) > 1:\n",
        "            mort_pred_binary = mort_pred_binary.flatten()\n",
        "\n",
        "        # Calculate batch-wise metrics for Mortality\n",
        "        mort_accuracy = np.mean((mort_labels_binary == mort_pred_binary))  # Binary accuracy\n",
        "        kappa_mort = cohen_kappa_score(mort_labels_binary, mort_pred_binary)  # Cohen's Kappa\n",
        "\n",
        "        # Calculate batch-wise metrics for LoS (Length of Stay) regression\n",
        "        mad_los = np.mean(np.abs(los_labels.detach().cpu().numpy() - los_pred.detach().cpu().numpy()))  # Mean Absolute Deviation (MAD)\n",
        "        mape_los = mean_absolute_percentage_error(los_labels.detach().cpu().numpy(), los_pred.detach().cpu().numpy())  # Mean Absolute Percentage Error (MAPE)\n",
        "        mse_los = mean_squared_error(los_labels.detach().cpu().numpy(), los_pred.detach().cpu().numpy())  # Mean Squared Error (MSE)\n",
        "        msle_los = mean_squared_error(np.log1p(los_labels.detach().cpu().numpy()), np.log1p(los_pred.detach().cpu().numpy()))  # Mean Squared Logarithmic Error (MSLE)\n",
        "        r2_los = r2_score(los_labels.detach().cpu().numpy(), los_pred.detach().cpu().numpy())  # R-squared (R2)\n",
        "\n",
        "        # Save metrics for the current batch\n",
        "        batch_metrics.append({\n",
        "            'Batch': batch_idx,\n",
        "            'MAD (LoS)': mad_los,\n",
        "            'MAPE (LoS)': mape_los,\n",
        "            'MSE (LoS)': mse_los,\n",
        "            'MSLE (LoS)': msle_los,\n",
        "            'R2 (LoS)': r2_los,\n",
        "            'Accuracy (Mortality)': mort_accuracy,\n",
        "            'Kappa (Mortality)': kappa_mort\n",
        "        })\n",
        "\n",
        "        # # Print batch-wise metrics\n",
        "        # print(f\"Batch {batch_idx + 1}:\")\n",
        "        # print(f\"  LoS MAD: {mad_los:.4f}, MAPE: {mape_los:.4f}, MSE: {mse_los:.4f}, MSLE: {msle_los:.4f}, R2: {r2_los:.4f}\")\n",
        "        # print(f\"  Mortality Accuracy: {mort_accuracy:.4f}, Kappa: {kappa_mort:.4f}\\n\")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        # print(f\"Skipping batch {batch_idx + 1} due to error: {e}\")\n",
        "        continue  # Skip this batch if there's an error (e.g., mismatched tensor shapes)\n",
        "\n",
        "# Convert batch-wise metrics to DataFrame\n",
        "batch_metrics_df = pd.DataFrame(batch_metrics)\n",
        "\n",
        "# Aggregate metrics across all batches (e.g., taking the mean)\n",
        "aggregated_metrics = {\n",
        "    'MAD (LoS)': batch_metrics_df['MAD (LoS)'].mean(),\n",
        "    'MSE (LoS)': batch_metrics_df['MSE (LoS)'].mean(),\n",
        "    'MSLE (LoS)': batch_metrics_df['MSLE (LoS)'].mean(),\n",
        "    'R2 (LoS)': batch_metrics_df['R2 (LoS)'].mean(),\n",
        "    'Accuracy (Mortality)': batch_metrics_df['Accuracy (Mortality)'].mean(),\n",
        "    'Kappa (Mortality)': batch_metrics_df['Kappa (Mortality)'].mean()\n",
        "}\n",
        "\n",
        "\n",
        "# Optionally, display batch-wise metrics as well\n",
        "print(aggregated_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "USE NEW LOSS FUNCTION"
      ],
      "metadata": {
        "id": "T_H0l38w0W_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config[\"loss\"] = \"hdloss\""
      ],
      "metadata": {
        "id": "kMKjWE4T0Zjb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TempPointConv(\n",
        "    config=config,\n",
        "    F=train_datareader.F,\n",
        "    D=train_datareader.D,\n",
        "    no_flat_features= train_datareader.no_flat_features\n",
        "    ).to(device=device)"
      ],
      "metadata": {
        "id": "DwYa8OoO0t87"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5gzIcun0yWt",
        "outputId": "d93ca7c2-2268-4170-88fb-7dff6472997e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TempPointConv(\n",
              "  (relu): ReLU()\n",
              "  (sigmoid): Sigmoid()\n",
              "  (hardtanh): Hardtanh(min_val=0.020833333333333332, max_val=100)\n",
              "  (msle_loss): MSLELoss(\n",
              "    (squared_error): MSELoss()\n",
              "  )\n",
              "  (mse_loss): MSELoss(\n",
              "    (squared_error): MSELoss()\n",
              "  )\n",
              "  (bce_loss): BCELoss()\n",
              "  (hb_loss): HybridLoss(\n",
              "    (mse_loss): MSELoss()\n",
              "    (mae_loss): L1Loss()\n",
              "  )\n",
              "  (main_dropout): Dropout(p=0.45, inplace=False)\n",
              "  (temp_dropout): Dropout(p=0.05, inplace=False)\n",
              "  (empty_module): EmptyModule()\n",
              "  (diagnosis_encoder): Linear(in_features=293, out_features=64, bias=True)\n",
              "  (bn_diagnosis_encoder): MyBatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn_point_last_los): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn_point_last_mort): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (point_final_los): Linear(in_features=17, out_features=1, bias=True)\n",
              "  (point_final_mort): Linear(in_features=17, out_features=1, bias=True)\n",
              "  (layer_modules): ModuleDict(\n",
              "    (0): ModuleDict(\n",
              "      (temp): Conv1d(174, 1044, kernel_size=(4,), stride=(1,), groups=87)\n",
              "      (bn_temp): MyBatchNorm1d(1044, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=241, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): ModuleDict(\n",
              "      (temp): Conv1d(1300, 1200, kernel_size=(4,), stride=(1,), dilation=(3,), groups=100)\n",
              "      (bn_temp): MyBatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=1298, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): ModuleDict(\n",
              "      (temp): Conv1d(1469, 1356, kernel_size=(4,), stride=(1,), dilation=(6,), groups=113)\n",
              "      (bn_temp): MyBatchNorm1d(1356, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=1454, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): ModuleDict(\n",
              "      (temp): Conv1d(1638, 1512, kernel_size=(4,), stride=(1,), dilation=(9,), groups=126)\n",
              "      (bn_temp): MyBatchNorm1d(1512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=1610, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): ModuleDict(\n",
              "      (temp): Conv1d(1807, 1668, kernel_size=(4,), stride=(1,), dilation=(12,), groups=139)\n",
              "      (bn_temp): MyBatchNorm1d(1668, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=1766, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): ModuleDict(\n",
              "      (temp): Conv1d(1976, 1824, kernel_size=(4,), stride=(1,), dilation=(15,), groups=152)\n",
              "      (bn_temp): MyBatchNorm1d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=1922, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (6): ModuleDict(\n",
              "      (temp): Conv1d(2145, 1980, kernel_size=(4,), stride=(1,), dilation=(18,), groups=165)\n",
              "      (bn_temp): MyBatchNorm1d(1980, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=2078, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (7): ModuleDict(\n",
              "      (temp): Conv1d(2314, 2136, kernel_size=(4,), stride=(1,), dilation=(21,), groups=178)\n",
              "      (bn_temp): MyBatchNorm1d(2136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=2234, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (8): ModuleDict(\n",
              "      (temp): Conv1d(2483, 2292, kernel_size=(4,), stride=(1,), dilation=(24,), groups=191)\n",
              "      (bn_temp): MyBatchNorm1d(2292, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (point): Linear(in_features=2390, out_features=13, bias=True)\n",
              "      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (point_last_los): Linear(in_features=2781, out_features=17, bias=True)\n",
              "  (point_last_mort): Linear(in_features=2781, out_features=17, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches = train_datareader.batch_gen(batch_size=config[\"batch_size\"])"
      ],
      "metadata": {
        "id": "uZvMQyPf1rsf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Mean Squared Error for Length of Stay (LoS) prediction (Regression Task)\n",
        "criterion_los = nn.MSELoss()\n",
        "\n",
        "# Binary Cross-Entropy with Logits for Mortality prediction (Binary Classification Task)\n",
        "criterion_mort = nn.BCEWithLogitsLoss()\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def train(model, train_datareader, optimizer, criterion_los, criterion_mort, device):\n",
        "    model.train()\n",
        "    running_loss_los = 0.0\n",
        "    running_loss_mort = 0.0\n",
        "    num_batches = 0  # To count the number of batches\n",
        "\n",
        "    # Iterate directly over the generator produced by batch_gen\n",
        "    for batch in train_datareader.batch_gen(batch_size=config['batch_size']):\n",
        "        try:\n",
        "            padded, mask, diagnoses, flat, los_labels, mort_labels, seq_lengths = batch  # Unpack correctly\n",
        "\n",
        "            # Move data to the device (GPU/CPU)\n",
        "            padded, los_labels, mort_labels = padded.to(device), los_labels.to(device), mort_labels.to(device)\n",
        "            flat, diagnoses = flat.to(device), diagnoses.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            los_pred, mort_pred = model(padded, diagnoses, flat)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss_los = criterion_los(los_pred, los_labels)\n",
        "            loss_mort = criterion_mort(mort_pred, mort_labels)\n",
        "\n",
        "            # Total loss\n",
        "            loss = loss_los + loss_mort\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss_los += loss_los.item()\n",
        "            running_loss_mort += loss_mort.item()\n",
        "            num_batches += 1  # Increment the batch counter\n",
        "            # print(f'Batch {num_batches}: LoS Loss: {loss_los.item():.4f}, Mortality Loss: {loss_mort.item():.4f}')\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"Skipping batch due to error: {e}\")\n",
        "            continue  # Skip this batch and proceed to the next one\n",
        "\n",
        "    # Calculate the average loss\n",
        "    avg_loss_los = running_loss_los / num_batches if num_batches > 0 else 0\n",
        "    avg_loss_mort = running_loss_mort / num_batches if num_batches > 0 else 0\n",
        "\n",
        "    return avg_loss_los, avg_loss_mort\n",
        "\n",
        "\n",
        "# Example of training for one epoch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# for epoch in range(10):  # Run for 10 epochs as an example\n",
        "train_loss_los, train_loss_mort = train(model, train_datareader, optimizer, criterion_los, criterion_mort, device)\n",
        "print(f' LoS Loss: {train_loss_los:.4f}, Mortality Loss: {train_loss_mort:.4f}')\n"
      ],
      "metadata": {
        "id": "znsfedPd1wiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0d5c84-f87a-4ae3-abf1-c58952ee2fbe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LoS Loss: 7.1269, Mortality Loss: 0.7014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
        "\n",
        "# Store per-batch metrics\n",
        "batch_metrics = []\n",
        "\n",
        "# Iterate through the validation data\n",
        "for batch_idx, batch in enumerate(val_datareader.batch_gen(batch_size=config['batch_size'])):\n",
        "    padded, mask, diagnoses, flat, los_labels, mort_labels, seq_lengths = batch\n",
        "\n",
        "    # Move data to device (GPU/CPU)\n",
        "    padded, los_labels, mort_labels = padded.to(device), los_labels.to(device), mort_labels.to(device)\n",
        "    flat, diagnoses = flat.to(device), diagnoses.to(device)\n",
        "\n",
        "    try:\n",
        "        # Forward pass: Get model predictions\n",
        "        los_pred, mort_pred = model(padded, diagnoses, flat)\n",
        "\n",
        "        # Ensure binary predictions (0 or 1) by thresholding the predicted values\n",
        "        mort_pred_binary = (mort_pred.detach().cpu().numpy() > 0.5).astype(int)  # Apply threshold to get binary (0 or 1)\n",
        "\n",
        "        # Ensure labels are binary (0 or 1)\n",
        "        mort_labels_binary = mort_labels.detach().cpu().numpy().astype(int)\n",
        "\n",
        "        # Flatten if the predictions or labels are not 1D arrays\n",
        "        if len(mort_labels_binary.shape) > 1:\n",
        "            mort_labels_binary = mort_labels_binary.flatten()\n",
        "\n",
        "        if len(mort_pred_binary.shape) > 1:\n",
        "            mort_pred_binary = mort_pred_binary.flatten()\n",
        "\n",
        "        # Calculate batch-wise metrics for Mortality\n",
        "        mort_accuracy = np.mean((mort_labels_binary == mort_pred_binary))  # Binary accuracy\n",
        "        kappa_mort = cohen_kappa_score(mort_labels_binary, mort_pred_binary)  # Cohen's Kappa\n",
        "\n",
        "        # Calculate batch-wise metrics for LoS (Length of Stay) regression\n",
        "        mad_los = np.mean(np.abs(los_labels.detach().cpu().numpy() - los_pred.detach().cpu().numpy()))  # Mean Absolute Deviation (MAD)\n",
        "        mape_los = mean_absolute_percentage_error(los_labels.detach().cpu().numpy(), los_pred.detach().cpu().numpy())  # Mean Absolute Percentage Error (MAPE)\n",
        "        mse_los = mean_squared_error(los_labels.detach().cpu().numpy(), los_pred.detach().cpu().numpy())  # Mean Squared Error (MSE)\n",
        "        msle_los = mean_squared_error(np.log1p(los_labels.detach().cpu().numpy()), np.log1p(los_pred.detach().cpu().numpy()))  # Mean Squared Logarithmic Error (MSLE)\n",
        "        r2_los = r2_score(los_labels.detach().cpu().numpy(), los_pred.detach().cpu().numpy())  # R-squared (R2)\n",
        "\n",
        "        # Save metrics for the current batch\n",
        "        batch_metrics.append({\n",
        "            'Batch': batch_idx,\n",
        "            'MAD (LoS)': mad_los,\n",
        "            'MAPE (LoS)': mape_los,\n",
        "            'MSE (LoS)': mse_los,\n",
        "            'MSLE (LoS)': msle_los,\n",
        "            'R2 (LoS)': r2_los,\n",
        "            'Accuracy (Mortality)': mort_accuracy,\n",
        "            'Kappa (Mortality)': kappa_mort\n",
        "        })\n",
        "\n",
        "        # # Print batch-wise metrics\n",
        "        # print(f\"Batch {batch_idx + 1}:\")\n",
        "        # print(f\"  LoS MAD: {mad_los:.4f}, MAPE: {mape_los:.4f}, MSE: {mse_los:.4f}, MSLE: {msle_los:.4f}, R2: {r2_los:.4f}\")\n",
        "        # print(f\"  Mortality Accuracy: {mort_accuracy:.4f}, Kappa: {kappa_mort:.4f}\\n\")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        # print(f\"Skipping batch {batch_idx + 1} due to error: {e}\")\n",
        "        continue  # Skip this batch if there's an error (e.g., mismatched tensor shapes)\n",
        "\n",
        "# Convert batch-wise metrics to DataFrame\n",
        "batch_metrics_df = pd.DataFrame(batch_metrics)\n",
        "\n",
        "# Aggregate metrics across all batches (e.g., taking the mean)\n",
        "aggregated_metrics = {\n",
        "    'MAD (LoS)': batch_metrics_df['MAD (LoS)'].mean(),\n",
        "    'MSE (LoS)': batch_metrics_df['MSE (LoS)'].mean(),\n",
        "    'MSLE (LoS)': batch_metrics_df['MSLE (LoS)'].mean(),\n",
        "    'R2 (LoS)': batch_metrics_df['R2 (LoS)'].mean(),\n",
        "    'Accuracy (Mortality)': batch_metrics_df['Accuracy (Mortality)'].mean(),\n",
        "    'Kappa (Mortality)': batch_metrics_df['Kappa (Mortality)'].mean()\n",
        "}\n",
        "\n",
        "\n",
        "# Optionally, display batch-wise metrics as well\n",
        "print(aggregated_metrics)"
      ],
      "metadata": {
        "id": "9o3SFFu61y6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b53351-7534-49c3-c212-18f59d6f3253"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'MAD (LoS)': np.float32(0.6363913), 'MSE (LoS)': np.float64(10.93475916633706), 'MSLE (LoS)': np.float64(0.11792143061757088), 'R2 (LoS)': np.float64(0.058985815296215), 'Accuracy (Mortality)': np.float64(0.9175082806602836), 'Kappa (Mortality)': np.float64(0.38759203387716344)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdnawVFKOKSC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}